{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392936cc-3445-4ecf-8838-c0ebf7e73304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b130ebd-c269-47f7-9d0c-999297ac9541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-26T11_2019-01-27T11.csv with shape (49, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-27T11_2019-01-28T11.csv with shape (1634093, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-28T11_2019-01-29T11.csv with shape (936947, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-29T11_2019-01-30T11.csv with shape (682724, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-30T11_2019-01-31T11.csv with shape (761778, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-01-31T11_2019-02-01T11.csv with shape (496127, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-01T11_2019-02-02T11.csv with shape (322248, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-02T11_2019-02-03T11.csv with shape (813916, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-03T11_2019-02-04T11.csv with shape (3215389, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-04T11_2019-02-05T11.csv with shape (482750, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-05T11_2019-02-06T11.csv with shape (561748, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-06T11_2019-02-07T11.csv with shape (515913, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-07T11_2019-02-08T11.csv with shape (550415, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-08T11_2019-02-09T11.csv with shape (350254, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\video_sent\\video_sent_2019-02-09T11_2019-02-10T11.csv with shape (572708, 15)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-26T11_2019-01-27T11.csv with shape (49, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-27T11_2019-01-28T11.csv with shape (9203951, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-28T11_2019-01-29T11.csv with shape (5960228, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-29T11_2019-01-30T11.csv with shape (4175577, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-30T11_2019-01-31T11.csv with shape (4653297, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-01-31T11_2019-02-01T11.csv with shape (3115628, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-01T11_2019-02-02T11.csv with shape (2008512, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-02T11_2019-02-03T11.csv with shape (4834563, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-03T11_2019-02-04T11.csv with shape (22219025, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-04T11_2019-02-05T11.csv with shape (3089684, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-05T11_2019-02-06T11.csv with shape (3626017, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-06T11_2019-02-07T11.csv with shape (3302159, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-07T11_2019-02-08T11.csv with shape (3332670, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-08T11_2019-02-09T11.csv with shape (2113379, 9)\n",
      "Loaded c:\\Users\\richa\\Desktop\\CodingWorkspaces\\bruh\\293N-YT-ABR-Prediction\\puffer_tests\\client_buffer\\client_buffer_2019-02-09T11_2019-02-10T11.csv with shape (3517582, 9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Base directory for datasets\n",
    "current_dir = os.getcwd()\n",
    "base_path = os.path.join(current_dir, 'puffer_tests')\n",
    "\n",
    "# Function to load and concatenate all CSVs in a folder\n",
    "def load_all_csvs(folder_path, source_label):\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "    df_list = []\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, dtype=str)\n",
    "            df['source_dataset'] = source_label\n",
    "            df_list.append(df)\n",
    "            print(f\"Loaded {file_path} with shape {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    if df_list:\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Empty DataFrame if nothing loaded\n",
    "\n",
    "# Load and concatenate video_sent CSVs\n",
    "video_sent_path = os.path.join(base_path, 'video_sent')\n",
    "data = load_all_csvs(video_sent_path, 'video_sent')\n",
    "\n",
    "# Load and concatenate client_buffer CSVs\n",
    "client_buffer_path = os.path.join(base_path, 'client_buffer')\n",
    "buffer_data = load_all_csvs(client_buffer_path, 'client_buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76210207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the time column is numeric\n",
    "data['time (ns GMT)'] = pd.to_numeric(data['time (ns GMT)'], errors='coerce')\n",
    "buffer_data['time (ns GMT)'] = pd.to_numeric(buffer_data['time (ns GMT)'], errors='coerce')\n",
    "\n",
    "# Drop rows with null time values (from coercion or original data)\n",
    "data = data.dropna(subset=['time (ns GMT)', 'session_id'])\n",
    "buffer_data = buffer_data.dropna(subset=['time (ns GMT)', 'session_id'])\n",
    "\n",
    "# Sort both dataframes properly\n",
    "data_sorted = data.sort_values(['time (ns GMT)', 'session_id']).reset_index(drop=True)\n",
    "buffer_sorted = buffer_data.sort_values(['time (ns GMT)', 'session_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76ae6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                time (ns GMT)                                    session_id  \\\n",
      "0         1548500400788000000  kaXlFjXCNjAgx4nsvYsDetENDzv04n/e7R4HYutFWt0=   \n",
      "1         1548500401993000000  j1E2fJfiOe5TU3Fj3lC1cgA4CsKjg2NJYBcxdVcdJfk=   \n",
      "2         1548500402853000000  kaXlFjXCNjAgx4nsvYsDetENDzv04n/e7R4HYutFWt0=   \n",
      "3         1548500402957000000  j1E2fJfiOe5TU3Fj3lC1cgA4CsKjg2NJYBcxdVcdJfk=   \n",
      "4         1548500404848000000  kaXlFjXCNjAgx4nsvYsDetENDzv04n/e7R4HYutFWt0=   \n",
      "...                       ...                                           ...   \n",
      "11897054  1549796398277000000  XWTlvZkd85BqdqcXwyqVf6IY7F36iSIT5fyZbF8OKrc=   \n",
      "11897055  1549796398504000000  E1Bxmt1OoguWfV4KRmIWTHtDxuD/gDhoa93gf1GczaE=   \n",
      "11897056  1549796399405000000  WE7K4AsaLAlnlA/bvZt82D19ywRaGdzQJajYlkzsP4g=   \n",
      "11897057  1549796399520000000  hGBfytiLEcfvK4VprTgO/26q3iLMIl9TzcPgXCixQlo=   \n",
      "11897058  1549796399613000000  J7QeGfG3JGbFvuJTbUbYE1OXFHcVbPhvFrmnq1qiEok=   \n",
      "\n",
      "         index_x expt_id_x channel_x     video_ts        format    size  \\\n",
      "0              0       232       abc  38279961720   1280x720-20  579932   \n",
      "1              0       237       abc  38280322080   1280x720-20  269778   \n",
      "2              0       232       abc  38280141900   1280x720-20  475807   \n",
      "3              0       237       abc  38280502260   1280x720-20  322657   \n",
      "4              0       232       abc  38280322080   1280x720-20  269778   \n",
      "...          ...       ...       ...          ...           ...     ...   \n",
      "11897054      13       333       nbc  13279626360  1920x1080-24  505536   \n",
      "11897055       1       330       nbc  13286293020  1920x1080-22  631644   \n",
      "11897056      32       336       cbs  13285932660    854x480-26   88731   \n",
      "11897057       0       331       pbs  13282329060  1920x1080-22  374958   \n",
      "11897058       6       328       abc  13287013740   1280x720-20  532056   \n",
      "\n",
      "         ssim_index  cwnd  ...     rtt delivery_rate source_dataset_x index_y  \\\n",
      "0          0.981149   294  ...  109157        669806       video_sent       0   \n",
      "1          0.986832   846  ...   70263      10908953       video_sent       0   \n",
      "2          0.983754   295  ...  104773       2900000       video_sent       0   \n",
      "3          0.986481   846  ...   72590      10908953       video_sent       0   \n",
      "4          0.986832   296  ...  114102       3839571       video_sent       0   \n",
      "...             ...   ...  ...     ...           ...              ...     ...   \n",
      "11897054   0.978201   414  ...   97683       2892822       video_sent      13   \n",
      "11897055   0.988315    77  ...   53767       3027577       video_sent       1   \n",
      "11897056   0.942057    30  ...   44483        300894       video_sent      32   \n",
      "11897057   0.990096   944  ...   89058       9604543       video_sent       0   \n",
      "11897058   0.987311  1392  ...   83907       5036171       video_sent       6   \n",
      "\n",
      "         expt_id_y channel_y  event  buffer cum_rebuf source_dataset_y  \n",
      "0              232       abc  timer  15.079     0.369    client_buffer  \n",
      "1              237       abc  timer  13.752     0.232    client_buffer  \n",
      "2              232       abc  timer  14.826     0.369    client_buffer  \n",
      "3              237       abc  timer  14.766     0.232    client_buffer  \n",
      "4              232       abc  timer  14.827     0.369    client_buffer  \n",
      "...            ...       ...    ...     ...       ...              ...  \n",
      "11897054       333       nbc  timer  14.897     0.137    client_buffer  \n",
      "11897055       330       nbc  timer  14.901     0.217    client_buffer  \n",
      "11897056       336       cbs  timer   14.61     1.179    client_buffer  \n",
      "11897057       331       pbs  timer  14.333    11.008    client_buffer  \n",
      "11897058       328       abc  timer  14.762     0.193    client_buffer  \n",
      "\n",
      "[11897059 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge asof (requires full sort on the merge key)\n",
    "merged = pd.merge_asof(\n",
    "    data_sorted,\n",
    "    buffer_sorted,\n",
    "    on='time (ns GMT)',\n",
    "    by='session_id',\n",
    "    direction='backward',\n",
    "    tolerance=10**9 * 10  # 10 seconds\n",
    ")\n",
    "\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad06e3dc-1311-4844-a330-e157e09b8a8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expt_settings = []\n",
    "with open('expt_settings', 'r') as f:\n",
    "    for line in f:\n",
    "        # Skip line number if present\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Split off the line number if present (e.g., \"1 {json}\")\n",
    "        parts = line.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            _, json_str = parts\n",
    "        else:\n",
    "            json_str = parts[0]\n",
    "        obj = json.loads(json_str)\n",
    "        expt_settings.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d969d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time (ns GMT)', 'session_id', 'index_x', 'expt_id_x', 'channel_x',\n",
      "       'video_ts', 'format', 'size', 'ssim_index', 'cwnd', 'in_flight',\n",
      "       'min_rtt', 'rtt', 'delivery_rate', 'source_dataset_x', 'index_y',\n",
      "       'expt_id_y', 'channel_y', 'event', 'buffer', 'cum_rebuf',\n",
      "       'source_dataset_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = merged\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2584d165-3568-4887-8a86-52c227e06abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ABR_Separated\\linear_bba\\1.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\1.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\1.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\2.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\3.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\4.csv\n",
      "Saved: ABR_Separated\\linear_bba\\2.csv\n",
      "Saved: ABR_Separated\\linear_bba\\3.csv\n",
      "Saved: ABR_Separated\\mpc\\1.csv\n",
      "Saved: ABR_Separated\\mpc\\2.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\2.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\3.csv\n",
      "Saved: ABR_Separated\\pensieve\\1.csv\n",
      "Saved: ABR_Separated\\pensieve\\2.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\5.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\6.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\7.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\8.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\9.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\10.csv\n",
      "Saved: ABR_Separated\\linear_bba\\4.csv\n",
      "Saved: ABR_Separated\\linear_bba\\5.csv\n",
      "Saved: ABR_Separated\\mpc\\3.csv\n",
      "Saved: ABR_Separated\\mpc\\4.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\4.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\5.csv\n",
      "Saved: ABR_Separated\\pensieve\\3.csv\n",
      "Saved: ABR_Separated\\pensieve\\4.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\11.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\12.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\13.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\14.csv\n",
      "Saved: ABR_Separated\\linear_bba\\6.csv\n",
      "Saved: ABR_Separated\\linear_bba\\7.csv\n",
      "Saved: ABR_Separated\\mpc\\5.csv\n",
      "Saved: ABR_Separated\\mpc\\6.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\6.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\7.csv\n",
      "Saved: ABR_Separated\\pensieve\\5.csv\n",
      "Saved: ABR_Separated\\pensieve\\6.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\15.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\16.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\17.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\18.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\19.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\20.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\21.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\22.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\23.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\24.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\25.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\26.csv\n",
      "Saved: ABR_Separated\\linear_bba\\8.csv\n",
      "Saved: ABR_Separated\\linear_bba\\9.csv\n",
      "Saved: ABR_Separated\\mpc\\7.csv\n",
      "Saved: ABR_Separated\\mpc\\8.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\8.csv\n",
      "Saved: ABR_Separated\\robust_mpc\\9.csv\n",
      "Saved: ABR_Separated\\pensieve\\7.csv\n",
      "Saved: ABR_Separated\\pensieve\\8.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\27.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\28.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\29.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\30.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\31.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\32.csv\n",
      "Saved: ABR_Separated\\puffer_ttp\\33.csv\n"
     ]
    }
   ],
   "source": [
    "def split_dfs(data):\n",
    "    # Split the dataframe based on unique values in the 'category' column\n",
    "    return {group: group_df for group, group_df in data.groupby('expt_id')}\n",
    "\n",
    "def split_dfs_merged(data):\n",
    "    # Split the dataframe based on unique values in the 'category' column\n",
    "    return {group: group_df for group, group_df in data.groupby('expt_id_x')}\n",
    "\n",
    "split_dataframes = split_dfs_merged(data)\n",
    "\n",
    "# Create base output directory\n",
    "base_dir = \"ABR_Separated\"\n",
    "\n",
    "# Make sure the base directory exists\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to keep count of files saved per ABR\n",
    "abr_counts = {}\n",
    "# Now split_dfs is a dictionary where the key is the category, and the value is the corresponding DataFrame\n",
    "for key, sub_df in split_dataframes.items():\n",
    "    abr_name = expt_settings[int(key)-1][\"abr\"]\n",
    "    abr_dir = os.path.join(base_dir, abr_name)\n",
    "\n",
    "    # Make sure the ABR-specific directory exists\n",
    "    os.makedirs(abr_dir, exist_ok=True)\n",
    "\n",
    "    # Determine filename\n",
    "    count = abr_counts.get(abr_name, 0) + 1\n",
    "    abr_counts[abr_name] = count\n",
    "    filename = f\"{count}.csv\"\n",
    "\n",
    "    # Full path to save\n",
    "    file_path = os.path.join(abr_dir, filename)\n",
    "    sub_df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a651fa05-0ed5-416f-bf79-e12153c4a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV files...\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# Base path where all method folders are\n",
    "METHOD = \"pensieve\"\n",
    "base_path = f'./ABR_Separated/{METHOD}/'\n",
    "\n",
    "# Collect all merged CSVs\n",
    "all_dfs = []\n",
    "\n",
    "print(\"Loading CSV files...\")\n",
    "\n",
    "# Loop through each ABR method folder\n",
    "path_pattern = os.path.join(base_path, '*.csv')  # Match all CSVs under each method folder\n",
    "for file_path in glob(path_pattern):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        all_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6cfbfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining CSVs into one DataFrame...\n",
      "Total rows loaded: 2052559\n"
     ]
    }
   ],
   "source": [
    "# Combine all data\n",
    "print(\"Combining CSVs into one DataFrame...\")\n",
    "data = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"Total rows loaded: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a263123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920x1080-22    1137948\n",
      "1280x720-20      599289\n",
      "640x360-24        78306\n",
      "1280x720-26       49695\n",
      "426x240-26        32229\n",
      "1280x720-24       31270\n",
      "1920x1080-24      23083\n",
      "640x360-26        21544\n",
      "1280x720-22       19935\n",
      "854x480-24        18919\n",
      "854x480-26        14833\n",
      "854x480-22         6387\n",
      "Name: format, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"format\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved downsampled dataset to downsampled_format_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Downsample each \"format\" group based on the 20,000 rule\n",
    "downsampled_data = (\n",
    "    data.groupby(\"format\", group_keys=False)\n",
    "        .apply(lambda x: x.head(25000) if len(x) > 25000 else x)\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "downsampled_data.to_csv(\"downsampled_format_data.csv\", index=False)\n",
    "\n",
    "print(\"Saved downsampled dataset to downsampled_format_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
